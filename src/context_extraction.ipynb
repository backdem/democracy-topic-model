{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d2cba-dc83-4a65-acca-1ccbe77f7fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a190cd-ca65-42b0-9592-e8053df0940e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37231e4b-9793-4f35-9c26-d5ca85352ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed_text(input_text, tokenizer, model):\n",
    "    # Tokenization\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    # Padding\n",
    "    max_length = 20  # Maximum sequence length for BERT\n",
    "    padded_token_ids = token_ids[:max_length]\n",
    "    attention_mask = [1] * len(padded_token_ids)\n",
    "    \n",
    "    # Padding to match max_length\n",
    "    while len(padded_token_ids) < max_length:\n",
    "        padded_token_ids.append(0)\n",
    "        attention_mask.append(0)\n",
    "        \n",
    "    # Convert to PyTorch tensors\n",
    "    input_ids = torch.tensor([padded_token_ids])\n",
    "    attention_mask = torch.tensor([attention_mask])\n",
    "    \n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "    contextual_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "    \n",
    "    return contextual_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9869c84-240d-4f57-b21e-c0781df4fcbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def context_words(input_text, tokenizer, window_size):\n",
    "    # Tokenization\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "\n",
    "    # Find the position of the target word\n",
    "    target_word = \"walk\"\n",
    "    word_index = tokens.index(target_word)\n",
    "\n",
    "    # Set the window size for the context words\n",
    "    window_size = 2\n",
    "\n",
    "    # Calculate the starting and ending positions for the context window\n",
    "    start_pos = max(0, word_index - window_size)\n",
    "    end_pos = min(len(tokens) - 1, word_index + window_size)\n",
    "\n",
    "    # Extract the context words\n",
    "    context_words = tokens[start_pos:end_pos+1]\n",
    "\n",
    "    return context_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1192820-31d8-4d9b-b2f9-d2285dd36e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input text\n",
    "input_text1 = \"I love to go for a walk in the park. Parks are made for walking.\"\n",
    "input_text2 = \"parks are made for walking\"\n",
    "\n",
    "contextual_embeddings1 = embed_text(input_text1, tokenizer, model)\n",
    "contextual_embeddings2 = embed_text(input_text2, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8a946-0d2c-4f56-9891-8752d421e2b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Index of the word to analyze\n",
    "word_index1 = 6  # Example: \"walk\"\n",
    "\n",
    "# Get the contextualized representation for the word\n",
    "word_embedding1 = contextual_embeddings1[word_index1]\n",
    "\n",
    "word_index2 = 4 # park\n",
    "word_embedding2 = contextual_embeddings2[word_index2]\n",
    "\n",
    "# Print the contextualized representation\n",
    "# print(\"Contextualized representation of the word:\", word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6be9d9-8147-4c1b-a9a3-97e7b8a59622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "rep_1_np = word_embedding1.numpy()\n",
    "rep_2_np = word_embedding2.numpy()\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = 1 - cosine(rep_1_np, rep_2_np)\n",
    "\n",
    "# Print similarity score\n",
    "print(\"Cosine similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a53d4-9da5-41cf-846c-6e08f46bf15d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = context_words(input_text1, tokenizer, 2)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980945c9-08b4-47f7-b94c-5c8f332094b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6a38d-aec1-4f72-88b6-6acd7f31014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model for dependency parsing and named entity recognition\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example text\n",
    "text = \"Italian government officials must maintain integrity in their actions.\"\n",
    "window_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d20a38-2460-4f1e-8654-a893f122ee8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = [token.text for token in nlp.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372e4851-cbfe-4fde-81cf-d71342d0b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text with BERT\n",
    "#tokens = tokenizer.tokenize(text)\n",
    "# Convert tokens to BERT input format\n",
    "#input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# Add special tokens [CLS] and [SEP]\n",
    "#input_ids = [tokenizer.cls_token_id] + input_ids + [tokenizer.sep_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad8210-2e6a-4fa4-a6ea-d16a89239e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain contextualized representations using BERT\n",
    "#with torch.no_grad():\n",
    "#    inputs = torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension\n",
    "#    outputs = model(inputs)\n",
    "#    contextualized_reps = outputs.last_hidden_state.squeeze(0)  # Remove batch dimension\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667f922-9620-42d8-bb10-e365bf2bbfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dependency parse and named entities using spaCy\n",
    "doc = nlp(text)\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "dependency_parse = [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "\n",
    "# Identify the context of \"integrity\" and its relationship with \"government\"\n",
    "integrity_context = []\n",
    "for i, token in enumerate(tokens):\n",
    "    if token == 'integrity':\n",
    "        # Get the surrounding tokens within a window size\n",
    "        context_tokens = tokens[max(0, i - window_size): i] + tokens[i+1: i+window_size+1]\n",
    "        integrity_context = [context_token for context_token in context_tokens]\n",
    "        break\n",
    "\n",
    "print(\"Integrity context:\", integrity_context)\n",
    "print(\"Named Entities:\", entities)\n",
    "print(\"Dependency Parse:\", dependency_parse)\n",
    "#print(\"Contextualized representations:\", contextualized_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec8e688-c44e-4a88-86e2-31c9fbf6b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example text\n",
    "text = \"government officials must maintain integrity in their actions.\"\n",
    "\n",
    "# Process the text using the language model\n",
    "doc = nlp(text)\n",
    "\n",
    "subject_token = None\n",
    "\n",
    "for token in doc:\n",
    "    \n",
    "    if token.text.lower() == \"government\" and token.dep_ == \"nsubj\":\n",
    "       \n",
    "        subject_token = token\n",
    "        break\n",
    "\n",
    "linked_word = None\n",
    "if subject_token is not None:\n",
    "    for token in doc:\n",
    "        print(f'{token} {token.dep_} {token.head}')\n",
    "        if token.dep_ == \"dobj\" and token.head == subject_token:\n",
    "            linked_word = token\n",
    "            break\n",
    "\n",
    "if linked_word is not None:\n",
    "    print(\"Linked Word:\", linked_word.text)\n",
    "    print(\"Dependency Relation:\", linked_word.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffe6f9-4b42-4d84-a2a7-fed77e7404bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the desired context (e.g., \"government\" as the subject)\n",
    "desired_context = \"government\"\n",
    "\n",
    "# Initialize a counter for words with the desired context\n",
    "count = 0\n",
    "\n",
    "# Iterate over the tokens in the sentence\n",
    "for token in doc:\n",
    "    if token.text.lower() != desired_context:\n",
    "        print(f'{token} {token.head}')\n",
    "        if token.head.text.lower() == desired_context and token.dep_ != \"conj\":\n",
    "            count += 1\n",
    "\n",
    "print(\"Count of words with context '{}': {}\".format(desired_context, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66564f11-690f-4ead-9359-ec2e03f8d4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example sentence\n",
    "text = \"The government officials maintain integrity in their actions.\"\n",
    "\n",
    "# Target word\n",
    "target_word = \"integrity\"\n",
    "\n",
    "# Process the text using the language model\n",
    "doc = nlp(text)\n",
    "\n",
    "# Find the subject tokens and their compounds\n",
    "subject_tokens = []\n",
    "for token in doc:\n",
    "    if token.dep_ == \"nsubj\" or token.head.dep_ == \"nsubj\":\n",
    "        subject_tokens.extend([token] + list(token.children))\n",
    "        \n",
    "print(subject_tokens)\n",
    "\n",
    "# Find the subject token closest to the target word\n",
    "closest_subject_token = None\n",
    "min_distance = float('inf')\n",
    "\n",
    "for subject_token in subject_tokens:\n",
    "    distance = abs(subject_token.i - doc.vocab[target_word].orth)\n",
    "    if distance < min_distance:\n",
    "        closest_subject_token = subject_token\n",
    "        min_distance = distance\n",
    "\n",
    "print(\"Closest subject token to '{}' is '{}'\".format(target_word, closest_subject_token.text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14eb00-d33b-4b64-b915-3e53565c14dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
