{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410f797-940b-4892-addc-32fead299663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import utils\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer #PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Download stopwords and initialize stemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb37a62-831b-4131-9745-7e6dd9385234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_stemmed_tokens(data_list):\n",
    "    all_stems = []\n",
    "  \n",
    "    for text in data_list:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "        # Tokenize the sentence\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and perform stemming\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords and token.isalnum()]\n",
    "        stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "        all_stems += stems\n",
    "        \n",
    "    return all_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c859c43-5dfc-4f93-ab93-74ecbeb9f721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_stem_word_dict(data_list, stem_word_dict):\n",
    "    if not stem_word_dict:\n",
    "        stem_word_dict = {}\n",
    "    for text in data_list:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "        # Tokenize the text into individual words\n",
    "        words = word_tokenize(text)\n",
    "        # Retrieve the stem for each word\n",
    "        stems = [stemmer.stem(word) for word in words]\n",
    "        # Create a dictionary to associate each stem with its corresponding words       \n",
    "        for word, stem in zip(words, stems):\n",
    "            stem_word_dict.setdefault(stem, []).append(word)\n",
    "    \n",
    "    for key in stem_word_dict.keys():\n",
    "        vals = stem_word_dict[key]\n",
    "        stem_word_dict[key] = list(set(vals))        \n",
    "            \n",
    "    return stem_word_dict        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a33255-a9f4-4a46-8f81-eb80d77a1df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_words(data_list):\n",
    "    all_words = []\n",
    "    for text in data_list:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "        # Tokenize the text into individual words\n",
    "        words = word_tokenize(text)\n",
    "        all_words += words\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341dc9cd-2796-4b90-8e71-c8a85a33d212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ngrams(data_list, n):\n",
    "    words = get_words(data_list)\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram_str = ' '.join(words[i:i+n]).lower()\n",
    "        ngrams.append(ngram_str)\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0a600-ad92-43a5-af79-844b493ac3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dict_counts(tokens, stemmed_seeds):\n",
    "    counts = np.zeros(len(stemmed_seeds))\n",
    "    non_tracked_tokens = {}\n",
    "    # get corpus size from number of ngrams using the formula:\n",
    "    # corpus_size = no_ngrams + n - 1\n",
    "    n = len(tokens[0])\n",
    "    no_ngrams = len(tokens)\n",
    "    corpus_size = no_ngrams + n - 1\n",
    "    d = corpus_size/n\n",
    "    for token in tokens:\n",
    "        found = False\n",
    "        for i in range(len(stemmed_seeds)):\n",
    "            if token in stemmed_seeds[i]:\n",
    "                counts[i] += 1\n",
    "                found = True\n",
    "        if not found:\n",
    "            non_tracked_tokens.setdefault(token, 0)\n",
    "            non_tracked_tokens[token] += 1\n",
    "    norms = [c/d for c in counts if len(tokens) > 0]\n",
    "    return (counts, norms, non_tracked_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c004d382-2f71-43e0-baa0-e8ac172ed16b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vocab_counts(tokens):\n",
    "    vocab_counts = {}\n",
    "    for token in tokens:\n",
    "        if token not in vocab_counts.keys():\n",
    "            vocab_counts[token] = 1\n",
    "        else:\n",
    "            vocab_counts[token] += 1\n",
    "    return dict(sorted(vocab_counts.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6366631-0936-4626-ba78-373689478b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data setof all countries, years and sources\n",
    "data_file = '../data/all_countries_0.0.5.csv'\n",
    "countries, years, all_countries_data = utils.get_countries_data(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188af16a-10b5-457c-a9d3-a742ae72f4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # load dictionary of topics\n",
    "# # strcuture [{\"name\": TOPIC_NAME, \"words\": NGRAMS_OF_KEYWORDS}, ...]\n",
    "# dict_file = '../data/dict_3.json'\n",
    "# n = 1\n",
    "# dictionary,topics = utils.get_seed_lists(dict_file, n, exact=True )\n",
    "# dictionary = np.squeeze(list(dictionary))\n",
    "# ngram_dict = []\n",
    "# if n == 1:\n",
    "#     ngram_dict = [ list(get_stemmed_tokens(l)) for l in dictionary ]\n",
    "# else:\n",
    "#     ngram_dict = dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c288fa-3801-4e84-96d3-08378c04ffed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_one_ngram_results(dict_file, data_file, year):\n",
    "    results = {\"country\": [], \"no_words\": []}\n",
    "    countries, years, all_countries_data = utils.get_countries_data(data_file)\n",
    "    dictionary,topics = utils.get_seed_lists(dict_file, n, exact=True )\n",
    "    dictionary = np.squeeze(list(dictionary))\n",
    "    ngram_dict = []    \n",
    "    ngram_dict = [ list(get_stemmed_tokens(l)) for l in dictionary ]\n",
    "    \n",
    "    year = year\n",
    "    results = {\"country\": [], \"no_words\": []}\n",
    "    non_tracked_tokens = {}\n",
    "    stem_word_dict = {}\n",
    "    for i in range(len(ngram_dict)):\n",
    "        results[i+1] = []\n",
    "        \n",
    "    for country in countries:\n",
    "        df = all_countries_data\n",
    "        country_data = df[(df['year'] == year) & (df['country'] == country)]\n",
    "        country_data = country_data.reset_index(drop=True)\n",
    "\n",
    "        # Preprocessed tokens (list of strings)\n",
    "        data_list = np.squeeze(country_data[[\"sentence\"]].to_numpy())\n",
    "          # Count single ngram\n",
    "        tokens = get_stemmed_tokens(data_list)\n",
    "        \n",
    "        if(len(tokens) == 0):\n",
    "            continue\n",
    "       \n",
    "        stem_word_dict = get_stem_word_dict(data_list, stem_word_dict)\n",
    "        counts, norms, others = get_dict_counts(tokens, ngram_dict)    \n",
    "        non_tracked_tokens = dict(Counter(non_tracked_tokens) + Counter(others))   \n",
    "        if not norms:\n",
    "            continue    \n",
    "        results[\"country\"].append(country)\n",
    "        results[\"no_words\"].append(len(tokens))\n",
    "        for i in range(len(norms)):\n",
    "            results[i+1].append(norms[i])\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447e92d-460d-4c65-9933-9a4fd72d71cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ngram_results(n, dict_file, data_file, year):\n",
    "    if n == 1:\n",
    "        return get_one_ngram_results(dict_file, data_file, year)\n",
    "    results = {\"country\": [], \"no_words\": []}\n",
    "    dictionary,topics = utils.get_seed_lists(dict_file, n, exact=True )\n",
    "    dictionary = np.squeeze(list(dictionary))\n",
    "    ngram_dict = dictionary\n",
    "    for i in range(len(ngram_dict)):\n",
    "        results[i+1] = []\n",
    "    for country in countries:\n",
    "        df = all_countries_data\n",
    "        country_data = df[(df['year'] == year) & (df['country'] == country)]\n",
    "        country_data = country_data.reset_index(drop=True)\n",
    "\n",
    "        # Preprocessed tokens (list of strings)\n",
    "        data_list = np.squeeze(country_data[[\"sentence\"]].to_numpy())\n",
    "        ngrams = get_ngrams(data_list, n) \n",
    "\n",
    "        if(len(ngrams) == 0):\n",
    "            continue\n",
    "        counts, norms, others = get_dict_counts(ngrams, ngram_dict)\n",
    "        if not norms:\n",
    "            continue    \n",
    "        results[\"country\"].append(country)\n",
    "        results[\"no_words\"].append(len(ngrams))\n",
    "        for i in range(len(norms)):\n",
    "            results[i+1].append(norms[i])\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80eb79-10e3-4936-ab00-baed0dc9003d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot(df):\n",
    "    nc = {}\n",
    "    for i in range(len(topics)):\n",
    "        nc[i+1] = topics[i]\n",
    "\n",
    "    df = df.rename(columns=nc)\n",
    "    # plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Iterate over each column (excluding the 'Category' column)\n",
    "    for col in df.columns[2:]:\n",
    "        fig.add_trace(go.Bar(x=df['country'], y=df[col], name=col))\n",
    "\n",
    "    # Customize the layout (optional)\n",
    "    fig.update_layout(title='Country Dimensions', xaxis_title='Country', yaxis_title='Coverage')\n",
    "\n",
    "    # Display the chart\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648bfeb-7e7a-48be-911e-85ef9c195204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for n in range(1, 4):    \n",
    "    results = get_ngram_results(n, \"../data/dict_3.json\", data_file, year=\"2020\")\n",
    "    df = pd.DataFrame(results)\n",
    "    plot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9478134c-4500-4810-8c41-7e1482294cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab051cf-cb56-422a-ad2b-58023d8e26c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_topic(df, \"corruption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f07408-1f6d-4a56-9b47-af50aa562217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot size\n",
    "df_sorted = df.sort_values(by='no_words', ascending=True)\n",
    "fig = go.Figure(data=[go.Bar(x=df_sorted['country'], y=df_sorted['no_words'])])\n",
    "\n",
    "# Customize the layout (optional)\n",
    "fig.update_layout(title='Corpus Size', xaxis_title='Country', yaxis_title='Size')\n",
    "\n",
    "# Display the chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531aa4c3-9195-499e-b174-16d5054c479a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc77a48-7a3a-4ab0-a85c-d9f7891fdc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
