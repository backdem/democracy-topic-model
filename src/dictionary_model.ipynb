{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410f797-940b-4892-addc-32fead299663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import utils\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer #PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import zscore\n",
    "from scipy import stats\n",
    "#import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_white, het_breuschpagan\n",
    "\n",
    "# Download stopwords and initialize stemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb37a62-831b-4131-9745-7e6dd9385234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_stemmed_tokens(data_list):\n",
    "    all_stems = []\n",
    "    for text in data_list:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "        # Tokenize the sentence\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and perform stemming\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords and token.isalnum()]\n",
    "        stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "        all_stems += stems\n",
    "        \n",
    "    return all_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd15df-13df-4fef-a7e1-201dfb4bf806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_stemmed_mapping(data_list):\n",
    "    all_stems = []\n",
    "    all_words = []\n",
    "    for text in data_list:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "        # Tokenize the sentence\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and perform stemming\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords and token.isalnum()]\n",
    "        stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "        all_stems += stems\n",
    "        all_words += filtered_tokens\n",
    "        \n",
    "    return (all_stems, all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c859c43-5dfc-4f93-ab93-74ecbeb9f721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_stem_word_dict(data_list, stem_word_dict):\n",
    "    if not stem_word_dict:\n",
    "        stem_word_dict = {}\n",
    "    for text in data_list:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "        # Tokenize the text into individual words\n",
    "        words = word_tokenize(text)\n",
    "        # Retrieve the stem for each word\n",
    "        stems = [stemmer.stem(word) for word in words]\n",
    "        # Create a dictionary to associate each stem with its corresponding words       \n",
    "        for word, stem in zip(words, stems):\n",
    "            stem_word_dict.setdefault(stem, []).append(word)\n",
    "    \n",
    "    for key in stem_word_dict.keys():\n",
    "        vals = stem_word_dict[key]\n",
    "        stem_word_dict[key] = list(set(vals))        \n",
    "            \n",
    "    return stem_word_dict        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a33255-a9f4-4a46-8f81-eb80d77a1df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_words(data_list):\n",
    "    all_words = []\n",
    "    for text in data_list:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "        # Tokenize the text into individual words\n",
    "        words = word_tokenize(text)\n",
    "        all_words += words\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341dc9cd-2796-4b90-8e71-c8a85a33d212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ngrams(data_list, n):\n",
    "    words = get_words(data_list)\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram_str = ' '.join(words[i:i+n]).lower()\n",
    "        ngrams.append(ngram_str)\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0a600-ad92-43a5-af79-844b493ac3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dict_counts(tokens, dictionary):\n",
    "    counts = np.zeros(len(dictionary))\n",
    "    token_counts = [ {} for i in range(len(dictionary)) ] \n",
    "    for token in tokens:\n",
    "        for i in range(len(dictionary)):\n",
    "            if token in dictionary[i]:\n",
    "                counts[i] += 1\n",
    "                token_counts[i][token] = token_counts[i].get(token, 0) + 1\n",
    "        \n",
    "    return ([c for c in counts if len(tokens) > 0], token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb39876-b783-490c-b473-8d31374758a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_stem_token_counts(tokens, words):\n",
    "    counts = np.zeros(len(tokens))\n",
    "    token_counts = [ {} for i in range(len(tokens)) ] \n",
    "    for token in tokens:\n",
    "        for i in range(len(dictionary)):\n",
    "            if token in dictionary[i]:\n",
    "                counts[i] += 1\n",
    "                token_counts[i][token] = token_counts[i].get(token, 0) + 1\n",
    "        \n",
    "    return ([c for c in counts if len(tokens) > 0], token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c004d382-2f71-43e0-baa0-e8ac172ed16b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vocab_counts(tokens):\n",
    "    vocab_counts = {}\n",
    "    for token in tokens:\n",
    "        if token not in vocab_counts.keys():\n",
    "            vocab_counts[token] = 1\n",
    "        else:\n",
    "            vocab_counts[token] += 1\n",
    "    return dict(sorted(vocab_counts.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe5644c-d610-4047-9417-f6ed4a911ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_json_dict(dict_file):\n",
    "    with open(dict_file, 'r') as file:\n",
    "        dictionary = json.load(file)\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b1739-5f97-45d6-8191-ed0a0812b3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data setof all countries, years and sources\n",
    "data_file = '../data/all_countries_0.0.6.csv'\n",
    "dict_file = \"../data/dict_6.json\"\n",
    "json_dict = load_json_dict(dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9120a3-6805-48a4-ba87-b0698fec9c86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_topic_name(n, dict_file=json_dict):\n",
    "    return json_dict[n-1][\"name\"]\n",
    "\n",
    "def get_no_topics():\n",
    "    return len(json_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7234b3c9-674e-4d28-b6ae-5436764fc3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "countries, years, all_countries_data, sources = utils.get_countries_data(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c288fa-3801-4e84-96d3-08378c04ffed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_one_ngram_results(dict_file, data_file, year):\n",
    "    results = {\"country\": [], \"no_words\": []}\n",
    "    countries, years, all_countries_data, sources = utils.get_countries_data(data_file)\n",
    "    dictionary,topics = utils.get_seed_lists(dict_file, 1, exact=True )\n",
    "    dictionary = np.squeeze(list(dictionary))\n",
    "    ngram_dict = []\n",
    "    # Stem the dictionary tokens\n",
    "    ngram_dict = [ list(get_stemmed_tokens(l)) for l in dictionary ]    \n",
    "    year = year\n",
    "    results = {\"country\": [], \"no_words\": []}\n",
    "    stem_word_dict = {}\n",
    "    for i in range(len(ngram_dict)):\n",
    "        results[i+1] = []\n",
    "        \n",
    "    for country in countries:\n",
    "        df = all_countries_data\n",
    "        country_data = None\n",
    "        if(year == \"All\"):\n",
    "            country_data = df[df['country'] == country]\n",
    "        else:\n",
    "            country_data = df[(df['year'] == year) & (df['country'] == country)]\n",
    "        \n",
    "        country_data = country_data.reset_index(drop=True)\n",
    "\n",
    "        # Preprocessed tokens (list of strings)\n",
    "        data_list = np.squeeze(country_data[[\"sentence\"]].to_numpy())\n",
    "        # Count single ngram\n",
    "        tokens = get_stemmed_tokens(data_list)\n",
    "        \n",
    "        if(len(tokens) == 0):\n",
    "            continue     \n",
    "       \n",
    "        counts, token_counts = get_dict_counts(tokens, ngram_dict)           \n",
    "        results[\"country\"].append(country)\n",
    "        results[\"no_words\"].append(len(tokens))\n",
    "        for i in range(len(counts)):\n",
    "            results[i+1].append(counts[i])\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13136122-f7c0-40ba-8113-9651b0d79435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_one_ngram_results_by_year(dict_file, data_file):\n",
    "    results = {\"year\": [], \"no_words\": []}\n",
    "    token_counts_all_years = {}\n",
    "    countries, years, all_countries_data, sources = utils.get_countries_data(data_file)\n",
    "    dictionary,topics = utils.get_seed_lists(dict_file, 1, exact=True )\n",
    "    dictionary = np.squeeze(list(dictionary))\n",
    "    ngram_dict = []\n",
    "    # Stem the dictionary tokens\n",
    "    ngram_dict = [ list(get_stemmed_tokens(l)) for l in dictionary ]        \n",
    "    stem_word_dict = {}\n",
    "    for i in range(len(ngram_dict)):\n",
    "        results[i+1] = []\n",
    "        \n",
    "    for year in years:\n",
    "        df = all_countries_data\n",
    "        year_data = df[(df['year'] == year)]\n",
    "        year_data = year_data.reset_index(drop=True)\n",
    "\n",
    "        # Preprocessed tokens (list of strings)\n",
    "        data_list = np.squeeze(year_data[[\"sentence\"]].to_numpy())\n",
    "        # Count single ngram\n",
    "        tokens = get_stemmed_tokens(data_list)\n",
    "        \n",
    "        if(len(tokens) == 0):\n",
    "            continue     \n",
    "       \n",
    "        counts, token_counts = get_dict_counts(tokens, ngram_dict)           \n",
    "        results[\"year\"].append(year)\n",
    "        results[\"no_words\"].append(len(tokens))\n",
    "        token_counts_all_years[year] = token_counts\n",
    "        for i in range(len(counts)):\n",
    "            results[i+1].append(counts[i])\n",
    "            \n",
    "    return (results, token_counts_all_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41a954-b4a7-42d1-aa24-04096a92e809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_one_ngram_results_by_source(dict_file, data_file):\n",
    "    results = {\"source\": [], \"no_words\": []}\n",
    "    token_counts_all_source = {}\n",
    "    countries, years, all_countries_data, sources = utils.get_countries_data(data_file)\n",
    "    dictionary,topics = utils.get_seed_lists(dict_file, 1, exact=True )\n",
    "    dictionary = np.squeeze(list(dictionary))\n",
    "    ngram_dict = []\n",
    "    # Stem the dictionary tokens\n",
    "    ngram_dict = [ list(get_stemmed_tokens(l)) for l in dictionary ]        \n",
    "    stem_word_dict = {}\n",
    "    for i in range(len(ngram_dict)):\n",
    "        results[i+1] = []\n",
    "        \n",
    "    for source in sources:\n",
    "        df = all_countries_data\n",
    "        source_data = df[(df['source'] == source)]\n",
    "        source_data = source_data.reset_index(drop=True)\n",
    "\n",
    "        # Preprocessed tokens (list of strings)\n",
    "        data_list = np.squeeze(source_data[[\"sentence\"]].to_numpy())\n",
    "        # Count single ngram\n",
    "        tokens = get_stemmed_tokens(data_list)\n",
    "        \n",
    "        if(len(tokens) == 0):\n",
    "            continue     \n",
    "       \n",
    "        counts, token_counts = get_dict_counts(tokens, ngram_dict)           \n",
    "        results[\"source\"].append(source)\n",
    "        results[\"no_words\"].append(len(tokens))\n",
    "        token_counts_all_source[source] = token_counts\n",
    "        for i in range(len(counts)):\n",
    "            results[i+1].append(counts[i])\n",
    "            \n",
    "    return (results, token_counts_all_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84092003-712a-4a03-a642-a89ce2a3fdee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_one_ngram_results_by_source_and_country(dict_file, data_file):\n",
    "    results = {\"source\": [], \"country\": [], \"no_words\": []}\n",
    "    token_counts_all_source = {}\n",
    "    countries, years, all_countries_data, sources = utils.get_countries_data(data_file)\n",
    "    dictionary,topics = utils.get_seed_lists(dict_file, 1, exact=True )\n",
    "    dictionary = np.squeeze(list(dictionary))\n",
    "    ngram_dict = []\n",
    "    # Stem the dictionary tokens\n",
    "    ngram_dict = [ list(get_stemmed_tokens(l)) for l in dictionary ]        \n",
    "    stem_word_dict = {}\n",
    "    for i in range(len(ngram_dict)):\n",
    "        results[i+1] = []\n",
    "    for country in countries:\n",
    "        for source in sources:\n",
    "            df = all_countries_data\n",
    "            source_data = df[(df['source'] == source) & (df[\"country\"] == country)]\n",
    "            source_data = source_data.reset_index(drop=True)\n",
    "\n",
    "            # Preprocessed tokens (list of strings)\n",
    "            data_list = np.squeeze(source_data[[\"sentence\"]].to_numpy())\n",
    "            # Count single ngram\n",
    "            tokens = get_stemmed_tokens(data_list)\n",
    "\n",
    "            if(len(tokens) == 0):\n",
    "                continue     \n",
    "\n",
    "            counts, token_counts = get_dict_counts(tokens, ngram_dict)           \n",
    "            results[\"source\"].append(source)\n",
    "            results[\"country\"].append(country)\n",
    "            results[\"no_words\"].append(len(tokens))\n",
    "            token_counts_all_source[source] = token_counts\n",
    "            for i in range(len(counts)):\n",
    "                results[i+1].append(counts[i])\n",
    "\n",
    "    return (results, token_counts_all_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447e92d-460d-4c65-9933-9a4fd72d71cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ngram_results(n, dict_file, data_file, year):\n",
    "    if n == 1:\n",
    "        return get_one_ngram_results(dict_file, data_file, year)\n",
    "    results = {\"country\": [], \"no_words\": []}\n",
    "    dictionary,topics = utils.get_seed_lists(dict_file, n, exact=True )\n",
    "    dictionary = np.squeeze(list(dictionary))\n",
    "    ngram_dict = dictionary\n",
    "    for i in range(len(ngram_dict)):\n",
    "        results[i+1] = []\n",
    "    for country in countries:\n",
    "        df = all_countries_data\n",
    "        country_data = None\n",
    "        if(year == \"All\"):\n",
    "            country_data = df[df['country'] == country]\n",
    "        else:\n",
    "            country_data = df[(df['year'] == year) & (df['country'] == country)]\n",
    "            \n",
    "        country_data = country_data.reset_index(drop=True)\n",
    "\n",
    "        # Preprocessed tokens (list of strings)\n",
    "        data_list = np.squeeze(country_data[[\"sentence\"]].to_numpy())\n",
    "        ngrams = get_ngrams(data_list, n) \n",
    "\n",
    "        if(len(ngrams) == 0):\n",
    "            continue\n",
    "        counts, token_counts = get_dict_counts(ngrams, ngram_dict)\n",
    "         \n",
    "        results[\"country\"].append(country)\n",
    "        results[\"no_words\"].append(len(ngrams))\n",
    "        for i in range(len(counts)):\n",
    "            results[i+1].append(counts[i])\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d6871-d61e-4ba8-98f4-c21519307f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ngram_results_by_year(n, dict_file, data_file):\n",
    "    if n == 1:\n",
    "        return get_one_ngram_results_by_year(dict_file, data_file)\n",
    "    results = {\"year\": [], \"no_words\": []}\n",
    "    token_counts_all_years = {}\n",
    "    dictionary,topics = utils.get_seed_lists(dict_file, n, exact=True )\n",
    "    dictionary = np.squeeze(list(dictionary))\n",
    "    ngram_dict = dictionary\n",
    "    for i in range(len(ngram_dict)):\n",
    "        results[i+1] = []\n",
    "    for year in years:\n",
    "        df = all_countries_data\n",
    "        year_data = df[(df['year'] == year)]\n",
    "        year_data = year_data.reset_index(drop=True)\n",
    "\n",
    "        # Preprocessed tokens (list of strings)\n",
    "        data_list = np.squeeze(year_data[[\"sentence\"]].to_numpy())\n",
    "        ngrams = get_ngrams(data_list, n) \n",
    "\n",
    "        if(len(ngrams) == 0):\n",
    "            continue\n",
    "        counts, token_counts = get_dict_counts(ngrams, ngram_dict)\n",
    "         \n",
    "        results[\"year\"].append(year)\n",
    "        results[\"no_words\"].append(len(ngrams))\n",
    "        token_counts_all_years[year] = token_counts\n",
    "        for i in range(len(counts)):\n",
    "            results[i+1].append(counts[i])\n",
    "            \n",
    "    return (results, token_counts_all_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b791a1c-2fa6-4c8c-b1eb-95a4dd5d1e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ngram_results_by_source(n, dict_file, data_file):\n",
    "    if n == 1:\n",
    "        return get_one_ngram_results_by_source(dict_file, data_file)\n",
    "    results = {\"source\": [], \"no_words\": []}\n",
    "    token_counts_all_source = {}\n",
    "    dictionary,topics = utils.get_seed_lists(dict_file, n, exact=True )\n",
    "    dictionary = np.squeeze(list(dictionary))\n",
    "    ngram_dict = dictionary\n",
    "    for i in range(len(ngram_dict)):\n",
    "        results[i+1] = []\n",
    "    for source in sources:\n",
    "        df = all_countries_data\n",
    "        source_data = df[(df['source'] == source)]\n",
    "        source_data = source_data.reset_index(drop=True)\n",
    "\n",
    "        # Preprocessed tokens (list of strings)\n",
    "        data_list = np.squeeze(source_data[[\"sentence\"]].to_numpy())\n",
    "        ngrams = get_ngrams(data_list, n) \n",
    "\n",
    "        if(len(ngrams) == 0):\n",
    "            continue\n",
    "        counts, token_counts = get_dict_counts(ngrams, ngram_dict)\n",
    "         \n",
    "        results[\"source\"].append(source)\n",
    "        results[\"no_words\"].append(len(ngrams))\n",
    "        token_counts_all_source[source] = token_counts\n",
    "        for i in range(len(counts)):\n",
    "            results[i+1].append(counts[i])\n",
    "            \n",
    "    return (results, token_counts_all_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ac4f5-e2b7-403f-8398-64dd8cedf729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ngram_results_by_source_and_country(n, dict_file, data_file):\n",
    "    if n == 1:\n",
    "        return get_one_ngram_results_by_source_and_country(dict_file, data_file)\n",
    "    results = {\"source\": [], \"country\": [], \"no_words\": []}\n",
    "    token_counts_all_source = {}\n",
    "    dictionary,topics = utils.get_seed_lists(dict_file, n, exact=True )\n",
    "    dictionary = np.squeeze(list(dictionary))\n",
    "    ngram_dict = dictionary\n",
    "    for i in range(len(ngram_dict)):\n",
    "        results[i+1] = []\n",
    "    for country in countries:\n",
    "        for source in sources:\n",
    "            df = all_countries_data\n",
    "            source_data = df[(df['source'] == source) & (df['country'] == country)]\n",
    "            source_data = source_data.reset_index(drop=True)\n",
    "\n",
    "            # Preprocessed tokens (list of strings)\n",
    "            data_list = np.squeeze(source_data[[\"sentence\"]].to_numpy())\n",
    "            ngrams = get_ngrams(data_list, n) \n",
    "\n",
    "            if(len(ngrams) == 0):\n",
    "                continue\n",
    "            counts, token_counts = get_dict_counts(ngrams, ngram_dict)\n",
    "\n",
    "            results[\"source\"].append(source)\n",
    "            results[\"country\"].append(country)\n",
    "            results[\"no_words\"].append(len(ngrams))\n",
    "            token_counts_all_source[source] = token_counts\n",
    "            for i in range(len(counts)):\n",
    "                results[i+1].append(counts[i])\n",
    "            \n",
    "    return (results, token_counts_all_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1201db-6d0b-44c3-b152-95926f6fbb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_corpus_sizes(data_file, year):\n",
    "    results = {\"country\": [], \"no_words\": []}\n",
    "    for country in countries:\n",
    "        df = all_countries_data\n",
    "        country_data = df[(df['year'] == year) & (df['country'] == country)]\n",
    "        country_data = country_data.reset_index(drop=True)\n",
    "\n",
    "        # Preprocessed tokens (list of strings)\n",
    "        data_list = np.squeeze(country_data[[\"sentence\"]].to_numpy())\n",
    "        ngrams = get_ngrams(data_list, n)\n",
    "         \n",
    "        results[\"country\"].append(country)\n",
    "        results[\"no_words\"].append(len(ngrams))\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74a507-69f6-432d-9547-808d99848758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_column(df, col):\n",
    "    # Perform Min-Max normalization on the specified column\n",
    "    min_value = df[col].min()\n",
    "    max_value = df[col].max()\n",
    "\n",
    "    df[col] = (df[col] - min_value) / (max_value - min_value)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5094a-dbd7-4050-8037-b40c34f256f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_columns(df, cols):\n",
    "    norm_df = df\n",
    "    for i in cols:\n",
    "        norm_df = normalize_column(norm_df, i)\n",
    "    return norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80eb79-10e3-4936-ab00-baed0dc9003d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot(df, x='country', start_col=2, year=None):\n",
    "    #nc = {}\n",
    "    #for i in range(len(topics)):\n",
    "    #    nc[i+1] = topics[i]\n",
    "\n",
    "    #df = df.rename(columns=nc)\n",
    "    # plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Iterate over each column (excluding the 'Category' column)\n",
    "    for col in df.columns[2:]:\n",
    "        fig.add_trace(go.Bar(x=df[x], y=df[col], name=get_topic_name(col)))\n",
    "\n",
    "    # Customize the layout (optional)\n",
    "    fig.update_layout(title=f'dimensions by {x} year {year}', xaxis_title=x, yaxis_title='coverage')\n",
    "\n",
    "    # Display the chart\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb494c9f-b2d1-4763-b373-b65f38f696d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_years(df, x='year', start_col=2):\n",
    "    \n",
    "    df = df.sort_values(by='year', ascending=True)\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Iterate over each column (excluding the first n columns)\n",
    "    for col in df.columns[2:]:\n",
    "        fig.add_trace(go.Scatter(x=df[x], y=df[col], name=get_topic_name(col)))\n",
    "\n",
    "    # Customize the layout (optional)\n",
    "    fig.update_layout(title=f'dimensions by {x}', xaxis_title=x, yaxis_title='coverage')\n",
    "\n",
    "    # Display the chart\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d940e-8b5b-470e-b2eb-4864aa902ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_sources(df, x='source', start_col=2):    \n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Iterate over each column (excluding the 'Category' column)\n",
    "    for col in df.columns[2:]:\n",
    "        fig.add_trace(go.Bar(x=df[x], y=df[col], name=get_topic_name(col)))\n",
    "\n",
    "    # Customize the layout (optional)\n",
    "    fig.update_layout(title=f'dimensions by {x}', xaxis_title=x, yaxis_title='coverage')\n",
    "\n",
    "    # Display the chart\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d07af-8c64-4ef2-a55e-6829e3991ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_corpus_vs_topic(df, x='no_words', col=None, y_pred=[], topic_name='', title=''):\n",
    "    fig = go.Figure()    \n",
    "    fig.add_trace(go.Scatter(x=df[x], y=df[col], name=topic_name, mode='markers', text=df['country']))\n",
    "    # Plot linear regression model is given\n",
    "    if(len(y_pred) > 0):\n",
    "        fig.add_trace(go.Scatter(x=df[x], y=y_pred, name=f'linear fit'))\n",
    "    # Customize the layout (optional)\n",
    "    fig.update_layout(title=title, xaxis_title='corpus size', yaxis_title=f'{topic_name} counts')\n",
    "\n",
    "    # Display the chart\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4fb28-564c-4798-b411-97fb6373953b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_fitted_vs_residuals(df, x='no_words', col=None, topic_name='', title=''):\n",
    "    # Plot to check for heteroskedasticity in data set\n",
    "    fig = go.Figure()    \n",
    "    \n",
    "    X = sm.add_constant(df[x])\n",
    "    y = df[col]\n",
    "\n",
    "    model = sm.OLS(y,X).fit()\n",
    "\n",
    "    # Calculate residuals\n",
    "    residuals = y - model.predict(X)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=df[x], y=residuals, name=topic_name, mode='markers', text=df['country']))\n",
    "    fig.add_trace(go.Scatter(x=df[x], y=np.zeros(len(residuals)), name=f'predicted'))\n",
    "    # Customize the layout (optional)\n",
    "    fig.update_layout(title=title, xaxis_title='corpus size', yaxis_title=f'{topic_name} residuals')\n",
    "\n",
    "    # Display the chart\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ca354-8061-4dd2-ac69-051a2baa61d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_corpus_mean_deviation(df, x='country', col=None, topic_name='', title=''):\n",
    "    fig = go.Figure()    \n",
    "    X = sm.add_constant(df['no_words'])\n",
    "    for c in col:        \n",
    "        y = df[c]        \n",
    "        model = sm.OLS(y,X).fit()\n",
    "\n",
    "        # Calculate residuals\n",
    "        residuals = y - model.predict(X)\n",
    "        min_value = residuals.min()\n",
    "        max_value = residuals.max()\n",
    "\n",
    "        norm_residuals = 2 * (residuals - min_value) / (max_value - min_value) - 1\n",
    "        \n",
    "        \n",
    "        fig.add_trace(go.Bar(x=df[x], y=norm_residuals, name=get_topic_name(c)))\n",
    "    \n",
    "    # Customize the layout (optional)\n",
    "    fig.update_layout(title=title, xaxis_title='country', yaxis_title='residuals')\n",
    "\n",
    "    # Display the chart\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490fe75-64df-4c2f-aa2b-9859955affe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_topic(df, n):    \n",
    "    # plot single topic\n",
    "    df_sorted = df.sort_values(by=topic, ascending=True)\n",
    "    fig = go.Figure(data=[go.Bar(x=df_sorted['country'], y=df_sorted[topic])])\n",
    "\n",
    "    # Customize the layout (optional)\n",
    "    fig.update_layout(title=topic, xaxis_title='Country', yaxis_title=f'{get_topic_name(n)}')\n",
    "\n",
    "    # Display the chart\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ba058-c5cc-4f36-9f76-097d489610f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_z_score(df, n):    \n",
    "    # plot single z_score of topic\n",
    "    col_name = str(n) + \"_z_score\"\n",
    "    df_sorted = df.sort_values(by=col_name, ascending=True)\n",
    "    fig = go.Figure(data=[go.Bar(x=df_sorted['country'], y=df_sorted[col_name])])\n",
    "\n",
    "    # Customize the layout (optional)\n",
    "    name = get_topic_name(n) + \" z_score\"\n",
    "    fig.update_layout(title=topic, xaxis_title='Country', yaxis_title=name)\n",
    "\n",
    "    # Display the chart\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffc4367-991a-458f-822e-62043907c98d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_terms_in_dimension(freq, topic):\n",
    "    data = freq[topic]\n",
    "    # Sort the dictionary by values in descending order\n",
    "    sorted_data = sorted(data.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Extract sorted keys and values from the sorted dictionary\n",
    "    keys = [item[0] for item in sorted_data]\n",
    "    values = [item[1] for item in sorted_data]\n",
    "    log_values = [math.log(value) for value in values]\n",
    "    fig = go.Figure(data=[go.Bar(x=keys, y=log_values)])\n",
    "\n",
    "    # Add labels and title\n",
    "    fig.update_layout(\n",
    "        xaxis_title='keywords',\n",
    "        yaxis_title=f'{get_topic_name(topic)} log counts',\n",
    "        title=f'{get_topic_name(topic)} dimension'\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f23d56d-d1a4-4005-9a16-681a3c2c4fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_most_freq_terms_per_topic(tks):\n",
    "    topics = {}\n",
    "    for years in tks:\n",
    "        for year in years.keys():\n",
    "            for i in range(len(years[year])):\n",
    "                topic_id = i + 1  \n",
    "                data = years[year][i]\n",
    "                collector = topics.get(topic_id, {})\n",
    "                \n",
    "                for term in data.keys():\n",
    "                    count = data[term]\n",
    "                    collector[term] = collector.get(term, 0) + count\n",
    "                topics[topic_id] = collector\n",
    "                #max_key = max(data, key=lambda k: data[k])\n",
    "                #max_value = data[max_key]\n",
    "                #topic = results.get(i, {})\n",
    "                #for d in data:\n",
    "                #    topic[i] = topic.get(d, 0) + data[d]\n",
    "                #results[i] = topic\n",
    "                #print(f'{year} {get_topic_name(i+1)} {max_key} {max_value} {i+1}')\n",
    "    return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b4f54-28bd-496d-9bec-ed6f4db3f1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_mean_std(df, cols):\n",
    "    # Function to calculate z-score\n",
    "    def calculate_z_score(x, mean, std):\n",
    "        return (x - mean) / std\n",
    "    \n",
    "    for c in cols:\n",
    "        c_name_mean = str(c)+\"_mean\"\n",
    "        c_name_std = str(c)+\"_std\"\n",
    "        c_name_z_score = str(c)+\"_z_score\"\n",
    "        #df[c_name_mean] = df[c].mean()\n",
    "        #df[c_name_std] = df[c].std()\n",
    "        mean = df[c].mean()\n",
    "        std = df[c].std()\n",
    "        df[c_name_z_score] = df[c].apply(calculate_z_score, args=(mean, std))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47238e-ec31-4ba6-8fdb-c62e3185518e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_mean(df, c):\n",
    "  \n",
    "    \n",
    "    # Calculate the mean and standard deviation of 'Column1'\n",
    "    mean_column1 = df[c].mean()\n",
    "    std_column1 = df[c].std()\n",
    "    \n",
    "    print(f'mean: {mean_column1}, std: {std_column1}')\n",
    "\n",
    "    # Generate data for the normal curve\n",
    "    x = np.linspace(mean_column1 - 3*std_column1, mean_column1 + 3*std_column1, 100)\n",
    "    y = norm.pdf(x, mean_column1, std_column1)\n",
    "\n",
    "    # Create the plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        mode='lines',\n",
    "        line=dict(color='blue', width=2),\n",
    "        name='Normal Distribution'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Normal Distribution Curve',\n",
    "        xaxis_title='Value',\n",
    "        yaxis_title='Probability Density',\n",
    "        showlegend=False,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5299e0fe-f796-4b36-8c13-31d8a7d4ea3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_regression(df, x_name, y_name):\n",
    "    X = df[[x_name]]\n",
    "    y = df[y_name]\n",
    "   \n",
    "    model = LinearRegression()\n",
    "    model.fit(X,y)\n",
    "    y_pred= model.predict(X)   \n",
    "    return y_pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352e35b-2618-4a16-802a-6ec4aa6ae9c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_vec_mean_std_z_score(df, cols):\n",
    "    copy_df = df[cols].copy()\n",
    "    # Calculate the mean of the vector\n",
    "    mean_vector = copy_df.mean()\n",
    "\n",
    "    # Calculate the standard deviation of the vector\n",
    "    std_vector = copy_df.std()\n",
    "\n",
    "    # Calculate the Z-scores of the vector using the zscore() function\n",
    "    z_scores = zscore(copy_df)\n",
    "\n",
    "    # Convert the Z-scores array to a DataFrame with column names\n",
    "    z_scores_df = pd.DataFrame(z_scores, columns=cols)\n",
    "    \n",
    "    return(mean_vector, std_vector, z_scores_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92fd084-8b19-4c6d-816a-023b47cc27da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_results(dfs, start_from=2):\n",
    "    result_df = dfs[0].copy()\n",
    "    column_names_list = result_df.columns.tolist()[start_from:]\n",
    "    dfs_i = [[i+1] for i in range(len(column_names_list))]\n",
    "    # concat count columns into one result_df\n",
    "    for i in range(1, len(dfs)):\n",
    "        df = dfs[i]        \n",
    "        new_names = [ str(i) + \"_\" + str(n) for n in column_names_list ]\n",
    "        for j in range(len(new_names)):               \n",
    "                dfs_i[j].append(new_names[j])\n",
    "        result_df[new_names] = df[column_names_list].copy()\n",
    "        \n",
    "    # sum the counts togather and drop extra columns\n",
    "    for c in column_names_list:        \n",
    "        result_df[c] = result_df[dfs_i[c-1]].sum(axis=1)\n",
    "        result_df.drop(columns=dfs_i[c-1][1:], inplace=True)\n",
    " \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a0440-d56d-44cd-93a9-c73cef8eb7b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def breusch_pagan_test(df, x_name, y_name, alpha=0.05):\n",
    "    # Linear regression\n",
    "    X = sm.add_constant(df[x_name])  # Add a constant term for the intercept\n",
    "    y = df[y_name]\n",
    "\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    residuals = model.resid\n",
    "\n",
    "    # White's test\n",
    "    white_test = het_white(residuals, X)\n",
    "    print(\"White's test p-value:\", white_test[1])\n",
    "    if white_test[1] < alpha:\n",
    "        print(\"Heteroskedasticity present\")\n",
    "    else:\n",
    "        print(\"NO heteroskedasticity present\")\n",
    "\n",
    "    # Breusch-Pagan test\n",
    "    bp_test = het_breuschpagan(residuals, X)\n",
    "    print(\"Breusch-Pagan test p-value:\", bp_test[1])\n",
    "    if bp_test[1] < alpha:\n",
    "        print(\"Heteroskedasticity present\")\n",
    "    else:\n",
    "        print(\"NO heteroskedasticity present\")\n",
    "    \n",
    "    # Extract the coefficient of X and its standard error from the model summary\n",
    "    coef_x = model.params[x_name]\n",
    "    std_err_x = model.bse[x_name]\n",
    "\n",
    "    # Calculate the t-statistic for the coefficient of X\n",
    "    t_stat = coef_x / std_err_x\n",
    "\n",
    "    # Calculate the p-value associated with the t-statistic\n",
    "    p_value = model.pvalues[x_name]\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Coefficient of X:\", coef_x)\n",
    "    print(\"Standard error of coefficient of X:\", std_err_x)\n",
    "    print(\"t-statistic:\", t_stat)\n",
    "    print(\"p-value:\", p_value)\n",
    "\n",
    "    if p_value < alpha:\n",
    "        print(f\"Reject the null hypothesis: {x_name} has a significant effect on {get_topic_name(y_name)}.\")\n",
    "    else:\n",
    "        print(f\"Fail to reject the null hypothesis: There is not enough evidence to conclude that {x_name} has a significant effect on {get_topic_name(y_name)}.\")\n",
    "\n",
    "\n",
    "    return (white_test[1], bp_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25051cde-40e6-498e-bb95-3417c3b61f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_stats(data, confidence_level=0.95):\n",
    "    data_mean = np.mean(data)\n",
    "    data_std_dev = np.std(data)\n",
    "    # Calculate the critical value based on the confidence level and a normal distribution\n",
    "    critical_value = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "\n",
    "    # Calculate the standard error of the mean for a population\n",
    "    standard_error = data_std_dev / np.sqrt(len(data))\n",
    "\n",
    "    # Calculate the margin of error\n",
    "    margin_of_error = critical_value * standard_error\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    confidence_interval = (data_mean - margin_of_error, data_mean + margin_of_error)\n",
    "    return (confidence_interval, margin_of_error, data_mean, data_std_dev, critical_value, standard_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80b648-f9c4-4dd2-9054-9909c2c5dacc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count ngrams 1 to 4\n",
    "dfs = []\n",
    "#year = \"2020\"\n",
    "year = \"All\"\n",
    "for n in range(1, 5):    \n",
    "    results = get_ngram_results(n, dict_file, data_file, year=year)\n",
    "    df = pd.DataFrame(results)\n",
    "    dfs.append(df)\n",
    "    \n",
    "# merge all ngrams togather\n",
    "merged_df = merge_results(dfs)\n",
    "# save data frame\n",
    "merged_df.to_csv('dims_all_years_all_countries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5423c1-0ce5-4ec6-9666-08bdc420de58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_country_groups(merged_df, title='Democratic dimensions'):\n",
    "    # add new column group\n",
    "    #merged_df['group'] = 0\n",
    "    # plot by country groups\n",
    "\n",
    "    west_22 = ['greece', 'italy', 'ireland', 'liechtenstein', 'monaco', 'luxembourg', \n",
    "               'portugal', 'finland', 'austria', 'norway', 'france', 'denmark', \n",
    "               'spain', 'san-marino', 'switzerland', 'sweden', 'germany', 'united-kingdom', \n",
    "               'belgium', 'netherlands', 'iceland', 'andorra']\n",
    "    cee = ['slovenia', 'slovakia', 'latvia', 'poland', 'malta', \n",
    "           'lithuania', 'croatia', 'cyprus', 'hungary', \n",
    "           'estonia', 'denmark', 'romania', 'bulgaria', 'czechia']\n",
    "    eu_candidates = [ 'kosovo', 'moldova','ukraine', 'serbia', \n",
    "                     'north-macedonia', 'turkey', 'bosnia-herzegovina', 'montenegro', \n",
    "                     'albania','georgia']\n",
    "    non_eu = ['armenia', 'azerbaijan', 'belarus', 'russia'] \n",
    "\n",
    "\n",
    "    #merged_df.loc[merged_df['country'].isin(west_22), 'group'] = 1\n",
    "    #merged_df.loc[merged_df['country'].isin(cee), 'group'] = 2\n",
    "    #merged_df.loc[merged_df['country'].isin(eu_candidates), 'group'] = 3\n",
    "    #merged_df.loc[merged_df['country'].isin(non_eu), 'group'] = 4\n",
    "\n",
    "\n",
    "    group_4 = merged_df[merged_df['country'].isin(non_eu)]            \n",
    "    group_3 = merged_df[merged_df['country'].isin(eu_candidates)]\n",
    "    group_1 = merged_df[merged_df['country'].isin(west_22)]\n",
    "    group_2 = merged_df[merged_df['country'].isin(cee)]\n",
    "\n",
    "    groups = [group_1, group_2, group_3, group_4]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    # Create a bar trace for each group\n",
    "    category_names = ['west_eu','cee','eu_candidates', 'non_eu']\n",
    "    traces = []\n",
    "    for t in range(1,6):\n",
    "        vals_mean = [g[t].mean() for g in groups]\n",
    "        margin_error = [get_stats(g[t])[1] for g in groups]\n",
    "        trace = go.Bar(x=category_names,\n",
    "                       y=vals_mean,\n",
    "                       error_y=dict(type='data', array=margin_error, visible=True),\n",
    "                       name=get_topic_name(t))\n",
    "\n",
    "        traces.append(trace)\n",
    "    # Create the figure and add the bar traces\n",
    "    fig = go.Figure(data=traces)\n",
    "\n",
    "    # Update the layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='country groups',\n",
    "        yaxis_title='counts',\n",
    "        barmode='group'  # Set barmode to 'group' to create grouped bars\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    \n",
    "plot_country_groups(merged_df)\n",
    "    \n",
    "# statistical significance \n",
    "# Perform the two-sample t-test\n",
    "#topics = [1]\n",
    "#for topic in topics:\n",
    "#    for i, g in enumerate(groups):\n",
    "#        for k, j in enumerate(groups):\n",
    "#            if (i == k):\n",
    "#                continue\n",
    "#            t_stat, p_value = stats.ttest_ind(g[topic], j[topic])\n",
    "#            alpha = 0.05\n",
    "            # Print the results\n",
    "            #print(\"t-statistic:\", t_stat)\n",
    "            #print(\"p-value:\", p_value)\n",
    "\n",
    "#            if p_value < alpha:\n",
    "#                print(f\"Statistically significant difference between {category_names[i]} , {category_names[k]} on topic {get_topic_name(topic)} with alpha {alpha}.\")\n",
    "#                #print(\"Reject the null hypothesis: There is a statistically significant difference between the two groups.\")\n",
    "#            else:\n",
    "#                print(f\"NO statistically significant difference between {category_names[i]} , {category_names[k]} on topic {get_topic_name(topic)} with alpha {alpha}.\")\n",
    "                #print(\"Fail to reject the null hypothesis: There is no statistically significant difference between the two groups.\")\n",
    "        \n",
    "\n",
    "#t_stat, p_value = stats.ttest_ind(group_1, group_3)\n",
    "#alpha = 0.05\n",
    "# Print the results\n",
    "#print(\"t-statistic:\", t_stat)\n",
    "#print(\"p-value:\", p_value)\n",
    "\n",
    "#if p_value < alpha:\n",
    "#    print(\"Reject the null hypothesis: There is a statistically significant difference between the two groups.\")\n",
    "#else:\n",
    "#    print(\"Fail to reject the null hypothesis: There is no statistically significant difference between the two groups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5617016-de71-4f71-8726-dd507ede01d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count ngrams 1 to 4 by source and by country\n",
    "dfs = []\n",
    "\n",
    "for n in range(1, 5):    \n",
    "    results, token_counts = get_ngram_results_by_source_and_country(n, dict_file, data_file)\n",
    "    df = pd.DataFrame(results)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc27cc13-8097-4330-a778-e0bf1f6c8de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge all ngrams togather\n",
    "source_country_df = merge_results(dfs, start_from=3)\n",
    "# save data frame\n",
    "source_country_df.to_csv('source_and_country_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437ed77-f723-4320-be74-60cde3eae233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot grouped countries by sources\n",
    "\n",
    "for source in sources:\n",
    "    df = source_country_df[source_country_df['source'] == source]\n",
    "    plot_country_groups(df, title=f'Democratic dimensions from source: {source}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f8220-4d36-46c6-97d1-2b3c01f2f322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot per country\n",
    "for topic in range(1,5):\n",
    "    # liner regression model fit\n",
    "    y_pred = linear_regression(merged_df, 'no_words', topic)\n",
    "    plot_corpus_vs_topic(merged_df, col=topic, y_pred=y_pred, topic_name=get_topic_name(topic), title=f'{year} {get_topic_name(topic)} counts vs corpus')    \n",
    "    # plot residulas to check for heteroscedasticity\n",
    "    plot_fitted_vs_residuals(merged_df, col=topic, topic_name=get_topic_name(topic), title=f'{year} {get_topic_name(topic)} residuals')\n",
    "    breusch_pagan_test(merged_df, x_name=\"no_words\", y_name=topic)\n",
    "    \n",
    "plot_corpus_mean_deviation(merged_df, col=[1,2,3,4,5],  topic_name=get_topic_name(topic), title='Dimensions over all years')\n",
    "\n",
    "# testing for heteroskedasticity\n",
    "#X = sm.add_constant(merged_df['no_words'])\n",
    "#y = merged_df[topic]\n",
    "\n",
    "#model = sm.OLS(y,X).fit()\n",
    "\n",
    "# Calculate residuals\n",
    "#residuals = y - model.predict(X)\n",
    "#merged_df[str(topic)+\"_residuals\"] = residuals\n",
    "#plot_corpus_mean_deviation(merged_df, col=str(topic)+\"_residuals\", y_pred=np.zeros(len(residuals)), topic_name=get_topic_name(topic), title=f'{year} {get_topic_name(topic)} mean difference')\n",
    "\n",
    "# Get heteroskedasticity-consistent standard errors (HC3)\n",
    "# HC3 is one of the methods for robust standard errors\n",
    "#robust_se = model.get_robustcov_results(cov_type='HC3').bse\n",
    "\n",
    "#print(\"Coefficients:\")\n",
    "#print(model.params)\n",
    "#print(\"\\nHeteroskedasticity-consistent standard errors:\")\n",
    "#print(robust_se)\n",
    "#print(\"\\nP-values:\")\n",
    "#print(model.pvalues)\n",
    "#print(\"\\nConfidence Intervals at 95%\")\n",
    "#i = 0\n",
    "#lower_bound = model.params[i] - 1.96 * robust_se[i]\n",
    "#upper_bound = model.params[i] + 1.96 * robust_se[i]\n",
    "#print(lower_bound)\n",
    "#print(upper_bound)\n",
    "#print(\"\\nt-statistic\")\n",
    "#t_stat = model.tvalues[i]\n",
    "#p_value = model.pvalues[i]\n",
    "#print(f't-stat: {t_stat}, p-value: {p_value}')\n",
    "\n",
    "\n",
    "\n",
    "# mean, std, z_score = calc_vec_mean_std_z_score(merged_df, [\"no_words\", 1])\n",
    "# print(f'{mean} {std} {z_score}')\n",
    "\n",
    "#print(merged_df)\n",
    "# normalize the counts using min-max\n",
    "#df = normalize_columns(merged_df, [1,2,3,4])\n",
    "#plot(merged_df, year=year)\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd319d8f-20b5-4e64-8bfb-f6ae0d3c46a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count ngrams 1 to 4\n",
    "dfs = []\n",
    "tks = []\n",
    "for n in range(1, 5):    \n",
    "    results, token_counts = get_ngram_results_by_year(n, dict_file, data_file)\n",
    "    df = pd.DataFrame(results)\n",
    "    dfs.append(df)\n",
    "    tks.append(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f290d-4ab9-4e8f-a3bb-43d9eecbf851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plots by year\n",
    "merged_df_years = merge_results(dfs)\n",
    "merged_df_years.to_csv('dims_vs_years.csv', index=False)\n",
    "plot_years(merged_df_years, x=\"year\")\n",
    "#print(merged_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59d6f3-1245-44ff-89fa-bc9d9127ed03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count ngrams 1 to 4\n",
    "dfs = []\n",
    "tks = []\n",
    "for n in range(1, 5):    \n",
    "    results, token_counts = get_ngram_results_by_source(n, dict_file, data_file)\n",
    "    df = pd.DataFrame(results)\n",
    "    dfs.append(df)\n",
    "    tks.append(token_counts)\n",
    "    \n",
    "merged_df_sources = merge_results(dfs)\n",
    "merged_df_sources.to_csv('corpus_sources.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864aa4a-36ef-4acf-a7c3-192ac17fd032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plots by source\n",
    "\n",
    "#topic = 5\n",
    "#y_pred = linear_regression(merged_df, 'no_words', topic)\n",
    "#plot_corpus_vs_topic(merged_df, col=topic, y_pred=y_pred, topic_name=get_topic_name(topic))\n",
    "\n",
    "# mean, std, z_score = calc_vec_mean_std_z_score(merged_df, [\"no_words\", 1])\n",
    "# print(f'{mean} {std} {z_score}')\n",
    "\n",
    "#print(merged_df)\n",
    "# normalize the counts using min-max\n",
    "#df = normalize_columns(merged_df, [1,2,3,4])\n",
    "plot_sources(merged_df_sources, x=\"source\")\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c9194c-309a-4831-98cd-e07efa67f691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot most frequent tokens\n",
    "r = get_most_freq_terms_per_topic(tks)\n",
    "for i in range(get_no_topics()):\n",
    "    plot_terms_in_dimension(r, i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa4ff4-49ed-427d-b69f-717d9943f57a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66929bd-0c05-4d2e-9c66-01bbc1591c20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calc means and std\n",
    "df = calc_mean_std(merged_df, [1,2,3,4,5])\n",
    "topic = 5\n",
    "plot_z_score(df, topic)\n",
    "plot_topic(df, topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f07408-1f6d-4a56-9b47-af50aa562217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot size\n",
    "df = get_corpus_sizes(data_file, \"2022\")\n",
    "df_sorted = df.sort_values(by='no_words', ascending=True)\n",
    "fig = go.Figure(data=[go.Bar(x=df_sorted['country'], y=df_sorted['no_words'])])\n",
    "\n",
    "# Customize the layout (optional)\n",
    "fig.update_layout(title='Corpus Size', xaxis_title='Country', yaxis_title='Size')\n",
    "\n",
    "# Display the chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527aec7-9181-4f64-ab2b-37797fcdc7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats.norm.ppf((1 + confidence_level) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336338b9-67f5-40dd-99b4-a9222fb3c0af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
