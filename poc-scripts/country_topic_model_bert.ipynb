{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2296b8-7934-47c5-9881-d1fec99fcfb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "from functools import reduce\n",
    "import json\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "# import openai\n",
    "# from bertopic.representation import OpenAI\n",
    "from bertopic.representation import KeyBERTInspired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0905d-83f7-4c7b-8d7b-7ee473f78a76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"../config.ini\")\n",
    "#openai_key = config['DEFAULT']['openai_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0807f-114b-495c-bf6b-83bae17b8ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data setof all countries, years and sources\n",
    "data_file = '../data/all_countries_0.0.2.csv'\n",
    "all_countries_data = pd.read_csv(data_file, dtype={'year': str}, comment='#')\n",
    "# calculate sentence lengths\n",
    "all_countries_data[\"sentence_len\"] = all_countries_data[\"sentence\"].apply(lambda x: len(x.split()))\n",
    "# cast sentence column to string\n",
    "all_countries_data['sentence'] = all_countries_data['sentence'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3ff72-2293-4f83-a3d9-2e8b8599a5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose a country and year\n",
    "country = [\"malta\"]\n",
    "year = \"2020\"\n",
    "df = pd.DataFrame(all_countries_data)\n",
    "country_data = df[(df['year'] == year) & (df['country'].isin(country))]\n",
    "# reset index; needed for proper parsing by BERT\n",
    "country_data = country_data.reset_index(drop=True)\n",
    "corpus_size = len(country_data)\n",
    "number_of_words = reduce(lambda x, y: x + y, country_data[\"sentence_len\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216a7871-a36a-47c4-a1c0-f4ee00a26e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show length of corpus\n",
    "print(f\"corpus size is {corpus_size} sentences.\")\n",
    "print(f\"total number of words is  {number_of_words}.\")\n",
    "# show first 10 sentences\n",
    "print(country_data.iloc[:10]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eaad50-7304-4da7-adc0-f261b57017c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dictionary of topics\n",
    "# strcuture [{\"name\": TOPIC_NAME, \"words\": NGRAMS_OF_KEYWORDS}, ...]\n",
    "dict_file = '../data/dict_2.json'\n",
    "dictionary = None\n",
    "with open(dict_file, 'r') as file:\n",
    "     dictionary = json.load(file)\n",
    "    \n",
    "def get_seed_lists(dictionary, ngram_size):\n",
    "    # create list of topics wit max ngram_size\n",
    "    seeds = []\n",
    "    for topic in dictionary:\n",
    "        seed = [w for w in topic[\"words\"] if len(w.split()) <= ngram_size]\n",
    "        seeds.append(seed)\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352dfb03-6e69-4d47-ad87-f0b29bbf196f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = get_seed_lists(dictionary, 1)\n",
    "print(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d287b4bb-6eeb-42c2-8726-df431057009a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup gpt representation model\n",
    "# for openai we are block using rate limiter.\n",
    "# openai.api_key = openai_key\n",
    "# representation_model = OpenAI(model=\"gpt-3.5-turbo\", chat=True)\n",
    "\n",
    "# use other model\n",
    "representation_model = KeyBERTInspired()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d3de6-4c8c-440f-886c-ea81935d34e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load BERT model paraphrase-MiniLM-L3-v2 (multilingual) or all-MiniLM-L6-v2 (english)\n",
    "# setting min_topic_size to 7 and n_grams from 1 to 3\n",
    "# we need to explore these parameters. Other parameters:\n",
    "# https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html\n",
    "# guided topic modeling: https://maartengr.github.io/BERTopic/getting_started/guided/guided.html\n",
    "# seed_topic_list = [[\"corruption\"],\n",
    "#                   [\"elections\", \"election\", \"assembly\"],\n",
    "#                  [\"freedom\", \"liberty\"]]\n",
    "seed_topic_list = get_seed_lists(dictionary, 3)\n",
    "model = BERTopic(representation_model=representation_model, seed_topic_list=seed_topic_list, verbose=True, embedding_model='all-MiniLM-L6-v2', min_topic_size = 5, n_gram_range=(1, 3))\n",
    "# model = BERTopic(verbose=True, embedding_model='all-MiniLM-L6-v2', min_topic_size = 10, n_gram_range=(1, 3))\n",
    "# fit model to our data\n",
    "topics, _ = model.fit_transform(country_data.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7e4c7-e241-4b9e-8e20-c20dea2202f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate topic-document probability matrix\n",
    "# topic_distr, _ = model.approximate_distribution(country_data.sentence, min_similarity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c773029-2782-4fc0-856b-44c78b894175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get topic information\n",
    "info = model.get_topic_info()\n",
    "# normalize counts\n",
    "info[\"norm_count\"] = info[\"Count\"].apply(lambda x: x / corpus_size)\n",
    "test = reduce(lambda x, y: x+y, info[\"norm_count\"])\n",
    "# test should add to 1\n",
    "assert test == 1\n",
    "# print topic information\n",
    "print(f\"Number of topics: {len(info)}\")\n",
    "print(info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d31051-4232-4bce-b3a6-8dd2905cd655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize topic words/n_grams\n",
    "fig = model.visualize_barchart(top_n_topics=10)\n",
    "fig.write_image(\"./test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b60843-ec6b-43e8-af9b-bd4f516cb09a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9407c-6688-4102-a065-d6dee9f6c521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize topic clusters\n",
    "model.visualize_hierarchy(top_n_topics=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d0aa6-172f-4f6d-992b-64f33f5e8899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.visualize_distribution(topic_distr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8b352-cddc-43f4-924c-08867be5ce40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search topics close to our categories\n",
    "dimensions = []\n",
    "for cat in dictionary:\n",
    "    print(f'Closest topic to category: {cat[\"name\"]}.')\n",
    "    topics = cat[\"words\"]\n",
    "    max_similarity = 0\n",
    "    \n",
    "    for topic in topics:\n",
    "        similar_topics, similarities = model.find_topics(topic, top_n = 1)\n",
    "        # most_similar = similar_topics[0]\n",
    "        if similarities[0] > max_similarity:\n",
    "            max_similarity = similarities[0]\n",
    "            most_similar = similar_topics[0]\n",
    "            best_topic = topic\n",
    "    info = model.get_topic_info(most_similar)\n",
    "    # add normalized counts\n",
    "    normalized_count = info[\"Count\"][0] / corpus_size\n",
    "    record = []\n",
    "    record.append(cat[\"name\"])\n",
    "    record.append(most_similar)\n",
    "    record.append(info[\"Name\"][0])\n",
    "    record.append(best_topic)\n",
    "    record.append(max_similarity)\n",
    "    record.append(normalized_count)\n",
    "    record.append(model.get_topic(most_similar))\n",
    "    \n",
    "    \n",
    "    dimensions.append(record)\n",
    "\n",
    "    print(f\"Most Similar Topic Info: {model.get_topic(most_similar)}\")\n",
    "    print(f\"Most Similar Topic Number: {most_similar}\")\n",
    "    print(f\"Best seed match: {best_topic}\")\n",
    "    print(f\"Similarity Score: {max_similarity}\")\n",
    "    print(f\"Topic normalized count: {normalized_count}\")\n",
    "    print(f\"Topic info: {model.get_topic_info(most_similar)}\")\n",
    "    print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a6826-a39f-4a4e-a7b0-bc595f7496d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write csv\n",
    "file_name = country[0] + \"_\" + year + \"_dimenstions.csv\"\n",
    "with open(os.path.join(\"../data/\", file_name), mode=\"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"Dimension\", \"Topic_No\", \"Topic_Name\",  \"Best_Dict_Word_Match\", \"Similarity\", \"Normalized_Count\", \"Topic_Words\"])\n",
    "    for row in dimensions:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe4e0e-e2e4-4d4f-948c-187aa1af24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document info\n",
    "doc_info = model.get_document_info(country_data.sentence)\n",
    "# write csv\n",
    "file_name = country[0] + \"_\" + year + \"_sentences.csv\"\n",
    "with open(os.path.join(\"../data/\", file_name), mode=\"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"Sentence\", \"Topic_Name\", \"Topic_No\", \"Probability\"])\n",
    "    for document, topic, name, top_n_words, prob in zip(doc_info[\"Document\"], doc_info[\"Topic\"], doc_info[\"Name\"], doc_info[\"Top_n_words\"], doc_info[\"Probability\"]):\n",
    "        writer.writerow([document, name, topic, prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91f3e7-b4da-4403-82bf-6b2d8e9426ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save topic information\n",
    "info = model.get_topic_info()\n",
    "file_name = country[0] + \"_\" + year + \"_topics.csv\"\n",
    "with open(os.path.join(\"../data/\", file_name), mode=\"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"Topic_No\", \"Topic_Name\", \"Count\", \"Topic_Words\"])\n",
    "    for row in zip(info[\"Topic\"], info[\"Name\"], info[\"Count\"]):\n",
    "        if row[0] == -1:\n",
    "            continue\n",
    "        row = row + (model.get_topic(row[0]),)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e631d-bdb8-40df-9207-59f928ba3683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(f\"../data/{country[0]}_{year}\")\n",
    "\n",
    "# loading model\n",
    "# model=BERTopic.load(\"../data/file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3722dd4-3ab8-4a32-8164-049a6d5cef9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
